{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# om gann ganpataye namah\n",
    "\n",
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2b8016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All normalized datasets loaded\n",
      "\n",
      "FD001:\n",
      "  Train: (20631, 22)\n",
      "  Test: (13096, 20)\n",
      "\n",
      "FD002:\n",
      "  Train: (53759, 23)\n",
      "  Test: (33991, 21)\n",
      "\n",
      "FD003:\n",
      "  Train: (24720, 22)\n",
      "  Test: (16596, 20)\n",
      "\n",
      "FD004:\n",
      "  Train: (61249, 19)\n",
      "  Test: (41214, 17)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 0: LOAD NORMALIZED DATASETS\n",
    "\n",
    "NORMALIZED_DIR = \"data/normalised-clean-data\"\n",
    "\n",
    "# Load all normalized datasets\n",
    "normalized_dfs = {}\n",
    "\n",
    "for dataset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    train_path = os.path.join(NORMALIZED_DIR, f'train_{dataset}_normalized.csv')\n",
    "    test_path = os.path.join(NORMALIZED_DIR, f'test_{dataset}_normalized.csv')\n",
    "    \n",
    "    normalized_dfs[f'train_{dataset}'] = pd.read_csv(train_path)\n",
    "    normalized_dfs[f'test_{dataset}'] = pd.read_csv(test_path)\n",
    "\n",
    "print(\"✅ All normalized datasets loaded\\n\")\n",
    "\n",
    "# Display loaded data\n",
    "for dataset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    train_shape = normalized_dfs[f'train_{dataset}'].shape\n",
    "    test_shape = normalized_dfs[f'test_{dataset}'].shape\n",
    "    print(f\"{dataset}:\")\n",
    "    print(f\"  Train: {train_shape}\")\n",
    "    print(f\"  Test: {test_shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6872d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NORMALIZED DATA STRUCTURE\n",
      "================================================================================\n",
      "\n",
      "Columns: ['unit_id', 'time', 'op_setting_1', 'op_setting_2', 'op_setting_3', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21', 'RUL', 'regime', 'regime_id']\n",
      "\n",
      "First 5 rows:\n",
      "   unit_id  time  op_setting_1  op_setting_2  op_setting_3  sensor_2  \\\n",
      "0        1     1       -0.0007       -0.0004         100.0 -1.721725   \n",
      "1        1     2        0.0019       -0.0003         100.0 -1.061780   \n",
      "2        1     3       -0.0043        0.0003         100.0 -0.661813   \n",
      "3        1     4        0.0007        0.0000         100.0 -0.661813   \n",
      "4        1     5       -0.0019       -0.0002         100.0 -0.621816   \n",
      "\n",
      "   sensor_3  sensor_4  sensor_7  sensor_8  ...  sensor_12  sensor_13  \\\n",
      "0 -0.134255 -0.925936  1.121141 -0.516338  ...   0.334262  -1.058890   \n",
      "1  0.211528 -0.643726  0.431930 -0.798093  ...   1.174899  -0.363646   \n",
      "2 -0.413166 -0.525953  1.008155 -0.234584  ...   1.364721  -0.919841   \n",
      "3 -1.261314 -0.784831  1.222827  0.188048  ...   1.961302  -0.224597   \n",
      "4 -1.251528 -0.301518  0.714393 -0.516338  ...   1.052871  -0.780793   \n",
      "\n",
      "   sensor_14  sensor_15  sensor_17  sensor_20  sensor_21  RUL  \\\n",
      "0  -0.269071  -0.603816  -0.781710   1.348493   1.194427  191   \n",
      "1  -0.642845  -0.275852  -0.781710   1.016528   1.236922  190   \n",
      "2  -0.551629  -0.649144  -2.073094   0.739891   0.503423  189   \n",
      "3  -0.520176  -1.971665  -0.781710   0.352598   0.777792  188   \n",
      "4  -0.521748  -0.339845  -0.136018   0.463253   1.059552  187   \n",
      "\n",
      "                  regime  regime_id  \n",
      "0  -0.0007--0.0004-100.0          1  \n",
      "1   0.0019--0.0003-100.0          3  \n",
      "2   -0.0043-0.0003-100.0          0  \n",
      "3       0.0007-0.0-100.0          5  \n",
      "4  -0.0019--0.0002-100.0          1  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Data types:\n",
      "unit_id           int64\n",
      "time              int64\n",
      "op_setting_1    float64\n",
      "op_setting_2    float64\n",
      "op_setting_3    float64\n",
      "sensor_2        float64\n",
      "sensor_3        float64\n",
      "sensor_4        float64\n",
      "sensor_7        float64\n",
      "sensor_8        float64\n",
      "sensor_9        float64\n",
      "sensor_11       float64\n",
      "sensor_12       float64\n",
      "sensor_13       float64\n",
      "sensor_14       float64\n",
      "sensor_15       float64\n",
      "sensor_17       float64\n",
      "sensor_20       float64\n",
      "sensor_21       float64\n",
      "RUL               int64\n",
      "regime           object\n",
      "regime_id         int64\n",
      "dtype: object\n",
      "\n",
      "✅ Sensor columns identified: 14\n",
      "   ['sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# STEP 0B: INSPECT NORMALIZED DATA STRUCTURE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NORMALIZED DATA STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_fd001 = normalized_dfs['train_FD001']\n",
    "print(f\"\\nColumns: {list(train_fd001.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(train_fd001.head())\n",
    "print(f\"\\nData types:\\n{train_fd001.dtypes}\")\n",
    "\n",
    "# Get sensor columns\n",
    "sensor_cols = [col for col in train_fd001.columns if col.startswith('sensor_')]\n",
    "print(f\"\\n✅ Sensor columns identified: {len(sensor_cols)}\")\n",
    "print(f\"   {sensor_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f5121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 1: ROLLING STATISTICS (ROLLING MEAN & STD)\n",
    "\n",
    "\n",
    "print(\"STEP 1: TEMPORAL WINDOWING - ROLLING STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Window size for rolling statistics\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "print(f\"\\nWindow size: {WINDOW_SIZE} cycles\")\n",
    "print(\"\\nStrategy:\")\n",
    "print(\"  1. Group by unit_id to prevent data leakage between engines\")\n",
    "print(\"  2. Calculate rolling mean for each sensor (smooths noise)\")\n",
    "print(\"  3. Calculate rolling std for each sensor (measures volatility)\")\n",
    "print(\"  4. NaN values in first 9 cycles (less than window size)\\n\")\n",
    "\n",
    "# Dictionary to store feature-engineered datasets\n",
    "fe_dfs = {}\n",
    "\n",
    "for dataset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    print(\".................\")\n",
    "    \n",
    "    # Process training data\n",
    "    print(f\"  Processing training data...\")\n",
    "    train_df = normalized_dfs[f'train_{dataset}'].copy()\n",
    "    \n",
    "    # Create rolling mean features for each sensor\n",
    "    for sensor in sensor_cols:\n",
    "        rolling_mean_col = f'{sensor}_rolling_mean_{WINDOW_SIZE}'\n",
    "        train_df[rolling_mean_col] = train_df.groupby('unit_id')[sensor].transform(\n",
    "            lambda x: x.rolling(window=WINDOW_SIZE, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Create rolling std features for each sensor\n",
    "    for sensor in sensor_cols:\n",
    "        rolling_std_col = f'{sensor}_rolling_std_{WINDOW_SIZE}'\n",
    "        train_df[rolling_std_col] = train_df.groupby('unit_id')[sensor].transform(\n",
    "            lambda x: x.rolling(window=WINDOW_SIZE, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    print(f\"    ✅ Rolling mean features added: {len(sensor_cols)}\")\n",
    "    print(f\"    ✅ Rolling std features added: {len(sensor_cols)}\")\n",
    "    print(f\"    Total features created: {len(sensor_cols) * 2}\")\n",
    "    print(f\"    Train data shape: {train_df.shape}\")\n",
    "    \n",
    "    # Process test data\n",
    "    print(f\"\\n  Processing test data...\")\n",
    "    test_df = normalized_dfs[f'test_{dataset}'].copy()\n",
    "    \n",
    "    # Create rolling mean features for each sensor\n",
    "    for sensor in sensor_cols:\n",
    "        rolling_mean_col = f'{sensor}_rolling_mean_{WINDOW_SIZE}'\n",
    "        test_df[rolling_mean_col] = test_df.groupby('unit_id')[sensor].transform(\n",
    "            lambda x: x.rolling(window=WINDOW_SIZE, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Create rolling std features for each sensor\n",
    "    for sensor in sensor_cols:\n",
    "        rolling_std_col = f'{sensor}_rolling_std_{WINDOW_SIZE}'\n",
    "        test_df[rolling_std_col] = test_df.groupby('unit_id')[sensor].transform(\n",
    "            lambda x: x.rolling(window=WINDOW_SIZE, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    print(f\"    ✅ Rolling mean features added: {len(sensor_cols)}\")\n",
    "    print(f\"    ✅ Rolling std features added: {len(sensor_cols)}\")\n",
    "    print(f\"    Total features created: {len(sensor_cols) * 2}\")\n",
    "    print(f\"    Test data shape: {test_df.shape}\")\n",
    "    \n",
    "    # Store back in dictionary\n",
    "    fe_dfs[f'train_{dataset}'] = train_df\n",
    "    fe_dfs[f'test_{dataset}'] = test_df\n",
    "    \n",
    "    print(f\"\\n  ✅ {dataset} rolling features complete\")\n",
    "    print(f\"     New columns (sample): {[col for col in train_df.columns if 'rolling' in col][:4]}...\")\n",
    "\n",
    "print(\".............\")\n",
    "print(\"✅ STEP 1 COMPLETE: Rolling statistics calculated for all datasets\")\n",
    "print(\".............\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1B: VERIFY ROLLING FEATURES\n",
    "\n",
    "print(\"\\n................\")\n",
    "print(\"VERIFICATION: ROLLING FEATURES\")\n",
    "print(\"\\n................\")\n",
    "\n",
    "# Check one dataset in detail\n",
    "train_fd001_fe = fe_dfs['train_FD001']\n",
    "\n",
    "print(f\"\\nFD001 Training Data - After Rolling Features:\")\n",
    "print(f\"  Total columns: {len(train_fd001_fe.columns)}\")\n",
    "print(f\"  Original sensors: {len(sensor_cols)}\")\n",
    "print(f\"  Rolling mean features: {len(sensor_cols)}\")\n",
    "print(f\"  Rolling std features: {len(sensor_cols)}\")\n",
    "print(f\"  Other columns (op_settings, unit_id, time, regime_id, RUL): 9\")\n",
    "print(f\"  Total: {len(sensor_cols) + len(sensor_cols) + len(sensor_cols) + 9}\")\n",
    "\n",
    "# Show sample columns\n",
    "print(f\"\\nFirst 10 columns:\")\n",
    "print(train_fd001_fe.columns[:10].tolist())\n",
    "\n",
    "print(f\"\\nLast 10 columns:\")\n",
    "print(train_fd001_fe.columns[-10:].tolist())\n",
    "\n",
    "# Check for NaN values\n",
    "print(f\"\\nNaN check (first engine):\")\n",
    "engine_1_data = train_fd001_fe[train_fd001_fe['unit_id'] == 1]\n",
    "print(f\"  Engine 1 rows: {len(engine_1_data)}\")\n",
    "print(f\"  NaN in rolling_mean (first 5 rows): {engine_1_data.iloc[:5][f'{sensor_cols[0]}_rolling_mean_{WINDOW_SIZE}'].isna().sum()}\")\n",
    "print(f\"  NaN in rolling_mean (after row 10): {engine_1_data.iloc[10:][f'{sensor_cols[0]}_rolling_mean_{WINDOW_SIZE}'].isna().sum()}\")\n",
    "\n",
    "# Verify no inter-engine bleed\n",
    "print(f\"\\nNo inter-engine bleed check:\")\n",
    "print(f\"  Last value of Engine 1 rolling_mean: {engine_1_data.iloc[-1][f'{sensor_cols[0]}_rolling_mean_{WINDOW_SIZE}']:.4f}\")\n",
    "engine_2_data = train_fd001_fe[train_fd001_fe['unit_id'] == 2]\n",
    "print(f\"  First value of Engine 2 rolling_mean: {engine_2_data.iloc[0][f'{sensor_cols[0]}_rolling_mean_{WINDOW_SIZE}']:.4f}\")\n",
    "print(f\"  ✅ Values are independent (no bleed)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ ALL VERIFICATIONS PASSED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990a0b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regime_id column removed from all normalized datasets\n"
     ]
    }
   ],
   "source": [
    "# Remove regime_id column from normalized datasets\n",
    "for dataset in ['FD001', 'FD002', 'FD003', 'FD004']:\n",
    "    if 'regime_id' in normalized_dfs[f'train_{dataset}'].columns:\n",
    "        normalized_dfs[f'train_{dataset}'].drop('regime_id', axis=1, inplace=True)\n",
    "    \n",
    "    if 'regime_id' in normalized_dfs[f'test_{dataset}'].columns:\n",
    "        normalized_dfs[f'test_{dataset}'].drop('regime_id', axis=1, inplace=True)\n",
    "\n",
    "print(\"regime_id column removed from all normalized datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f700b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved comparison for FD001\n",
      "✅ Saved comparison for FD002\n",
      "✅ Saved comparison for FD003\n",
      "✅ Saved comparison for FD004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def visualize_rolling_comparison(dataset_list, input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    for ds in dataset_list:\n",
    "        path = os.path.join(input_dir, f\"train_{ds}_normalized.csv\")\n",
    "        if not os.path.exists(path): continue\n",
    "        \n",
    "        df = pd.read_csv(path)\n",
    "        # Select first engine and a representative sensor (e.g., sensor_11)\n",
    "        unit_id = df['unit_id'].min()\n",
    "        sample = df[df['unit_id'] == unit_id].sort_values('time')\n",
    "        sensor = 'sensor_11' \n",
    "        \n",
    "        # Calculate comparison windows\n",
    "        w10 = sample[sensor].rolling(window=10, min_periods=1).mean()\n",
    "        w15 = sample[sensor].rolling(window=15, min_periods=1).mean()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(sample['time'], sample[sensor], alpha=0.3, label='Raw Normalized', color='gray')\n",
    "        plt.plot(sample['time'], w10, label='Rolling Mean (W=10)', linewidth=2, color='blue')\n",
    "        plt.plot(sample['time'], w15, label='Rolling Mean (W=15)', linewidth=2, color='red')\n",
    "        \n",
    "        plt.title(f\"{ds} - {sensor} Smoothing Comparison (Engine {unit_id})\")\n",
    "        plt.xlabel(\"Cycle\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "        \n",
    "        save_path = os.path.join(output_dir, f\"{ds}_rolling_comp.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved comparison for {ds}\")\n",
    "\n",
    "# Execution\n",
    "visualize_rolling_comparison(\n",
    "    dataset_list=['FD001', 'FD002', 'FD003', 'FD004'],\n",
    "    input_dir=\"data/normalised-clean-data\",\n",
    "    output_dir=\"feature_plots/rolling_visuals\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001b6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
